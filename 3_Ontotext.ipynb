{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bachelorarbeit - FH Landhut -  Wrabel Ludwig - Teil 3\n",
    "\n",
    "## Verwendung des Datensatzes von 1_Date (Github) mit GraphDB von Ontotext\n",
    "\n",
    "* **Ziel**: mit den Titanic Datensätz aus der kaggle Herausforderung “Titanic: Maschinelles Lernen aus Katastrophen” alle dazugehörigen Schritte von der Installation einer Umgebung bis hin zum Auslesen von Daten aus GraphDB von Ontotext durchzuführen und zu dokumentieren. \n",
    "\n",
    "* **Fragestellung**: Ermöglicht Neo4j oder Ontotext den einfacheren Einstieg mit Python zum Ein-/Auslesen des kaggle Titanic Datensatzes in einem Jupyter Notebook? \n",
    "\n",
    "* **Vorgehen**: Um die Frage zu beantworten, wird der kaggle Titanic Datensatz mit Python in einem Jupyter Notebook auf einer virtuellen Maschine aufgearbeitet (Teil 1: 1_Data) und in Neo4j (Teil 2: 2_Neo4j) und Ontotext (Teil 3: 3_Ontotext) mit den Abfragesprachen Cypher und SPARQL ein- und ausgelesen.\n",
    "\n",
    "## **Teil 3**: \n",
    "1. GraphDB Software\n",
    "2. Notebook einrichten\n",
    "3. Requests\n",
    "4. Ergebnis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.GraphDB Software\n",
    "* GraphDB installieren\n",
    "* Repository erstellen\n",
    "* OntoRefine\n",
    "* csv2RDF\n",
    "* Daten impotieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GraphDB installieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die Installation von GraphDB Free ist das Ausfüllen eines Formulares auf https://www.ontotext.com/products/graphdb/graphdb-free/ nötigt. Nach dem Bestätigen des Formulars werden die Installationslinks per E-Mail zugesendet. \n",
    "\n",
    "Bei Windows wird die Version für Windows ausgewählt. Der Download startet automatisch. Per Klick auf das Installationspaket (.exe) wird die Installation ausgeführt und ein Icon auf dem Desktop erstellt. \n",
    "\n",
    "Bei Ubuntu muss der Link *deb based Linux* ausgewählt werden. Bei Pop-Up muss dann Speichern ausgewählt werden. Mit *sudo apt install /home/ba-graphdb/Downloads/graphdb-free-9.2.0.deb* wird die Installation der Software ausgeführt. Bei der Installation wird ein Icon, welches im Ubuntu Menu gespeichert wird, erstellt. Durch klicken auf das Icon wird GraphDB gestartet.\n",
    "\n",
    "GraphDB von Ontotext start über den Localhost und kann im Browser bedient werde."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repository erstellen\n",
    "In GraphDB von Ontotext bietet an mehrere Repository (Aufbewahrungsorte) zu erstelen. Dabei muss ein ID und eine Base URL eingegeben werden. Bei dieser Durchführunge werden für die ID *ba-graphdb* und für die Base URL *http://example.org/* verwendet. Die voreingestellten Auswahlen bleiben identisch. Nach dem das Repository erstellt wurde, muss es im Dashboard ausgewählt werden. In Abbildung 3.1 ist das Ergebnis zu sehen:\n",
    "\n",
    "<img src=\"./../04_Img/3_Ontotext/Repository Overview.jpeg\" alt=\"Repository Overview\" width=\"700\" height=\"300\"/>\n",
    "<br/>\n",
    "Abbilung 3.1: Repository Übersicht\n",
    "<br/>\n",
    "Quelle: Eigene Darstellung\n",
    "<br/>\n",
    "<br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OntoRefine\n",
    "OntoRefine wird in diesem Noteboke nur kurz erwähnt, aber nicht verwendet. Zur Transformieren von csv in ttl/rdf wurde ein Pythoncript geschrieben, welchen im nächsten Teilkapiel erläutert wird.\n",
    "\n",
    "\n",
    "##### Schritt 1\n",
    "Ontotext bietet mit OnotRefine die Möglichkeit Daten im csv-Format in die Software bzw. in deine Table von GraphDB hochzuladen. Die Daten werden in einer Tabele gespeichet und können im Anschluss bearbeitet, sowie auch in der SPARQL-Oberfläche abgefragt werden. Für den letzten erwähnten Vorgang ist Erfahrung in SPARQL nötwenig, um korrekte und ausführbare Requests zu schreiben. Die Abbildung 3.2 zeigt den Zustand nach dem Impotieren der Daten und dem erstellen eines Projekts. Durch das Klicken auf die Kopzeile (z.B FirstName) öffnet sich ein Dropdown bei dem verschiedene Bearbeitungsmöglichkeiten vorgeschlagen werde z.B zum Sotieren der Daten oder auch zum Bearbeiten der Zellen. \n",
    "<br/>\n",
    "<img src=\"./../04_Img/3_Ontotext/OntoRefine.jpeg\" alt=\"OntoRefine\" width=\"1000\" height=\"1000\"/>\n",
    "<br/>\n",
    "Abbilung 3.2: OntoRefine Übersicht\n",
    "<br/>\n",
    "Quelle: Eigene Darstellung\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "Wichtig: Die Daten sind nur in der Tabelle und nicht in der GraphDB verfügbar. Um die Daten für die GraphDB verfügbar zu machen ist der nächste Schritt nötig.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Schritt 2\n",
    "Bei der Auswahl des RDF-Buttons erscheint das RDF Settings Formular. Beim Formular muss die Bas IRI ***http://example.com/*** (die zuvor angegeben URL) und der Base IRI Prefix ***ex*** angeben werden. Die Veränderung der Vorauswahl dient zur Anpassung des RDFs und können bei einem eingen Projekt frei gewählt werden. Das Ergebnis ist in der Abbildung 3.3 ersichtlich. Bei *Run* kann die voreingestellte Abfrage ausgeführt werden, welche alle Daten abfragt und 100 Datenreihen ausgibt. In dieser Seite ist es möglich verschiedne Abfragen zu testen, um die dann über *Data -> Open in main SPARQL Editor* in der GraphDB zu übernehmen. **Weitere Schritte werden in dieser Arbeit in Bezug zu OntoRefine nicht ausgeführt, da es bei der Übernahme der Anfragen SPARQL Erfahrungen notwendig sind.** \n",
    "\n",
    "<br/>\n",
    "<img src=\"./../04_Img/3_Ontotext/OntoRefine_2.jpeg\" alt=\"OntoRefine nach dem Import\" width=\"1000\" height=\"1000\"/>\n",
    "<br/>\n",
    "Abbilung 3.3: OntoRefine nach Import\n",
    "<br/>\n",
    "Quelle: Eigene Darstellung\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### csv2RDF\n",
    "Das Resultat der Schwirigkeiten mit Ontofine ist ein csv zu ttl Converter (05_CSV2Tuple) auf Basis der Code Zeilen aus http://code.activestate.com/recipes/577423-convert-csv-to-xml/. Beim beim ttl-Format bestand die gleich Schwirigkeit die Daten zu verwenden wie bei der csv-Datei. Auf Basis des ttl-Converters wurd ein csv zu rdf Converter (04_CSV2RDF) geschrieben. Das Ausgegeben RDF kann dirket als Import für die GraphDB verwendet werden.\n",
    "\n",
    "Bei dem Ersten Schritt muss die csv-Datei und eine leere RDF-Datei angeben werden muss. Die csv-Datei wird im nächsten Schritt mit csv.reader(open()) ausgelesen und die rdf-Datei wird mit Schreibrechten geöffnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://code.activestate.com/recipes/577423-convert-csv-to-xml/\n",
    "\n",
    "import csv\n",
    "\n",
    "csvFile = './../02_Datasets/test_train.csv'\n",
    "rdfFile = './../02_Datasets/dataForGraphDB.rdf'\n",
    "\n",
    "csvData = csv.reader(open(csvFile))\n",
    "rdfData = open(rdfFile, 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die RDF Kopzeite wird in das RDF geschrieben. Zu beachten: *xmlns:ex=\"http://example.org/\"* ==> Prefix: ex und Base URL: http://example.org/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdfData.write(\n",
    "    '<rdf:RDF' + \"\\n\" +\n",
    "    '\txmlns:ex=\"http://example.org/\"' + \"\\n\" +\n",
    "    '\txmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"' + \"\\n\" +\n",
    "    '\txmlns:rdfs=\"http://www.w3.org/2000/01/rdf-schema#\"' + \"\\n\" +\n",
    "    '\txmlns:xsd=\"http://www.w3.org/2001/XMLSchema#\"> ' + \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Reihen aus der csv-Datei werden einzeln durchlaufen. Beim ersten Druchlauf werden die Tag anhand der Kopfreihe festgesetzt. Die Tags werden bei allen weiteren Durchläufen werdentet. Ab dem zweiten Druchlaufen werden die Daten im RDF-Format in die RDF-Datei geschrieben. Die letzte Zeile schreibt das Closetag in das RDF.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rowNum = 0\n",
    "for row in csvData:\n",
    "    if rowNum == 0:\n",
    "        tags = row\n",
    "        # replace spaces w/ underscores in tag names\n",
    "        for i in range(len(tags)):\n",
    "            tags[i] = tags[i].replace(' ', '_')\n",
    "    else: \n",
    "        rdfData.write('  <rdf:Description about=\"http://example.org/P' + row[0] + '\">' + \"\\n\")\n",
    "        for i in range(len(tags)):\n",
    "            rdfData.write('    <ex:' + tags[i] + '>' \\\n",
    "                          + row[i] + '</ex:' + tags[i] + '>' + \"\\n\")\n",
    "        rdfData.write('  </rdf:Description>' + \"\\n\")\n",
    "            \n",
    "    rowNum +=1\n",
    "\n",
    "rdfData.write('</rdf:RDF>' + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit dem Ausführen der nächten Zeile wird die RDF-Datei wieder geschlossen und steht für den Import für die GrapDB bereit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdfData.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Daten impotieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das *dataForGraphDB.rdf* kann nun über *Import -> RDF -> Upload RDF files* bei GraphDB hochgeladen werden. Über das Klicken auf Import-Botton, welcher bei Abbildung 3.4 nach dem Upload erscheint, öffnet sich ein Formular.\n",
    "\n",
    "<br/>\n",
    "<img src=\"./../04_Img/3_Ontotext/Import_1.jpeg\" alt=\"Übersicht nach dem RDF Upload\" width=\"600\" height=\"700\"/>\n",
    "<br/>\n",
    "Abbilung 3.4: Übersicht nach dem RDF Upload\n",
    "<br/>\n",
    "Quelle: Eigene Darstellung\n",
    "<br/>\n",
    "<br/>\n",
    "Bei dem Formular vgl. Abbildung 3.5 muss die Base IRI *http://example.org/* eingeben werden und dann der Import bestätigt werden. Danach ist der Import der Daten abgeschlossen und es ergibt sich die Möglichkeit über den Menupunkt SPARQL abfragen zu schreiben und auszuführen.\n",
    "\n",
    "<br/>\n",
    "<img src=\"./../04_Img/3_Ontotext/Import_2.jpeg\" alt=\"Import Formular\" width=\"500\" height=\"500\"/>\n",
    "<br/>\n",
    "Abbilung 3.5: Import Formular\n",
    "<br/>\n",
    "Quelle: Eigene Darstellung\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook einrichten\n",
    "* Imports einrichten\n",
    "* SPARQLWrapper\n",
    "* Test Request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports einrichten\n",
    "\n",
    "Als Erstes muss der python-sparqlwrapper installiert werden, damit wir über dieses Notebook auf die Ontotext Datenbank zugreifen können. Der Befehlt *sudo apt-get install -y python-sparqlwrapper* muss nur einmal zum Beginn ausgeführt werden.\n",
    "\n",
    "Mit dem SPARQLWrapper sind nur Request gegen die Datanbank möglich. Der Import der Daten über den SPARQLWrapper ist nicht möglich.\n",
    "\n",
    "Im nächsten Schritt wird SPARQLWrapper, JSON, N3 vom SPARQLWrapper impotierte, welche bei der Verbingung benötigt werden. Pandas wird importiert, um die Daten aus dem ersten Teil mit den in Daten aus Github zu vergleichen. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelle: https://github.com/RDFLib/sparqlwrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sudo apt-get install -y python-sparqlwrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON, N3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SPARQLWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der GrapDB Endpoint muss dem SPARQLWrapper mitgegeben werden. Erst muss, wie in Abbildung 3.2, auf auf das Icon für die URL geklickt werden und dann, wie in Abbildung 3.3, die URL kopiert werden, um die URL im Anschluss in den SPARQLWrapper als Endpoint einzufügen.\n",
    "\n",
    "<br/>\n",
    "<img src=\"./../04_Img/3_Ontotext/Endpoint_1.jpeg\" alt=\"Endpoint Step 1\" width=\"500\" height=\"300\"/>\n",
    "<br/>\n",
    "Abbilung 3.1: GraphDB Endpoint Step 1\n",
    "<br/>\n",
    "Quelle: Eigene Darstellung\n",
    "<br/>\n",
    "<br/>\n",
    "<img src=\"./../04_Img/3_Ontotext/Endpoint_2.jpeg\" alt=\"Endpoint Step 1\" width=\"600\" height=\"600\"/>\n",
    "<br/>\n",
    "Abbilung 3.1: GraphDB Endpoint Step 1\n",
    "<br/>\n",
    "Quelle: Eigene Darstellung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparql = SPARQLWrapper(\"http://192.168.2.109:7200/repositories/ba-graphdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Request\n",
    "1. Test Request wird in setQuery eingefügt\n",
    "2. Das Format (JSON) wird festgelegt und die Query ausgeführt. Das Ergebnis wird in *results* gespeichert\n",
    "3. Rsults wird Reihe für Reihe eingelesen mit einer Print ausgabe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparql.setQuery(\"\"\"\n",
    "PREFIX ex: <http://example.org/>\n",
    "PREFIX onto: <http://www.ontotext.com/>\n",
    "\n",
    "select * where { \n",
    " ?s ?p ?o .\n",
    "} limit 3\n",
    "\"\"\")\n",
    "\n",
    "sparql.setReturnFormat(JSON)\n",
    "results = sparql.query().convert()\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Ergebnis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
